{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff425f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph,START,END\n",
    "from typing import TypedDict\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import Literal\n",
    "from typing import TypedDict, Literal, Annotated\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2983a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0758ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "class classification_state(TypedDict):\n",
    "    description: str\n",
    "    class_category: str\n",
    "    class_category_reason: str\n",
    "    classification_prompt: str\n",
    "    benign_prompt: str\n",
    "    pathogenic_prompt: str\n",
    "    both_prompt: str\n",
    "    class_truth: str\n",
    "    ground_truth: list\n",
    "    response: Annotated[list[str], operator.add]\n",
    "    reason: str\n",
    "    class_iter: int\n",
    "    current_iter: int\n",
    "    max_iter: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1552f900",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGoogleGenerativeAI(model='gemini-2.5-flash',temperature=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b1736eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class DescriptionEvaluation(BaseModel):\n",
    "    evaluation: Literal['Pathogenic', 'Benign','Both'] = Field(..., description=\"ACMG Attribute class only accepted values are Pathogenic, Benign, Both\")\n",
    "    reason: str = Field(..., description=\"Reason for the classification.\")\n",
    "\n",
    "class ACMGEvaluation(BaseModel):\n",
    "    evaluation: list[str] = Field(..., description=\"ACMG Attribute\")\n",
    "    reason: str = Field(..., description=\"Reason for the classification.\")\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    prompt: str = Field(..., description=\"Attribute Prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97707a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = model.with_structured_output(DescriptionEvaluation)\n",
    "attribute_model = model.with_structured_output(ACMGEvaluation)\n",
    "prompt_model = model.with_structured_output(Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4829fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def description_classification(state: classification_state):\n",
    "    description= state['description']\n",
    "    messages = [\n",
    "    SystemMessage(content=\n",
    "    f\"\"\"you are a ACMG Attribute classifier here is some extra details: {state['classification_prompt']}\"\"\"\n",
    "     ),\n",
    "    HumanMessage(content=f\"\"\" Here is the given description:\n",
    "     {description}, \n",
    "        ########### respond only in the structure format:\n",
    "        evaluation: description=\"ACMG Attribute class only accepted values are Pathogenic, Benign, Both\n",
    "        feedback: Reason for the classification.\n",
    "    \"\"\")\n",
    "    ]\n",
    "    response = classification_model.invoke(messages)\n",
    "    return {\"class_category\":response.evaluation,\"class_category_reason\":response.reason}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55043fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benign_classification(state: classification_state):\n",
    "    description= state['description']\n",
    "    messages = [\n",
    "    SystemMessage(content=f\"{state['benign_prompt']}\"),\n",
    "    HumanMessage(content=f\"\"\" Here is the given description:\n",
    "     {description}, \n",
    "        ########### respond only in the structure format:\n",
    "        evaluation: list[str] = Field(..., description=\"ACMG Attribute\")\n",
    "    reason: str = Field(..., description=\"Reason for the classification.\")\n",
    "    \"\"\")\n",
    "    ]\n",
    "    response = attribute_model.invoke(messages)\n",
    "    return {\"response\":response.evaluation,\"reason\":response.reason}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c28e8365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathogenic_classification(state: classification_state):\n",
    "    description= state['description']\n",
    "    messages = [\n",
    "    SystemMessage(content=f\"{state['pathogenic_prompt']}\"),\n",
    "    HumanMessage(content=f\"\"\" Here is the given description:\n",
    "     {description}, \n",
    "        ########### respond only in the structure format:\n",
    "        evaluation: list[str] = Field(..., description=\"ACMG Attribute\")\n",
    "    reason: str = Field(..., description=\"Reason for the classification.\")\n",
    "    \"\"\")\n",
    "    ]\n",
    "    response = attribute_model.invoke(messages)\n",
    "    return {\"response\":response.evaluation,\"reason\":response.reason}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9e9cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def both_classification(state: classification_state):\n",
    "    description= state['description']\n",
    "    messages = [\n",
    "    SystemMessage(content=f\"{state['both_prompt']}\"),\n",
    "    HumanMessage(content=f\"\"\" Here is the given description:\n",
    "     {description}, \n",
    "        ########### respond only in the structure format:\n",
    "        evaluation: list[str] = Field(..., description=\"ACMG Attribute\")\n",
    "    reason: str = Field(..., description=\"Reason for the classification.\")\n",
    "    \"\"\")\n",
    "    ]\n",
    "    response = attribute_model.invoke(messages)\n",
    "    return {\"response\":response.evaluation,\"reason\":response.reason}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51c39272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_check(state: classification_state):\n",
    "    class_check = {\"next\":\"\"}\n",
    "    if state['class_iter']>state['max_iter']:\n",
    "        class_check[\"next\"] = \"continue\"\n",
    "    else:\n",
    "        if state['class_category'] != state['class_truth']:\n",
    "            class_check[\"next\"] = \"check\"\n",
    "        else:\n",
    "            class_check[\"next\"] = \"continue\"\n",
    "    return class_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "da3d7d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_check(state: classification_state):\n",
    "    if state['current_iter'] > state['max_iter']:\n",
    "        return {\"next\": \"end\"}\n",
    "    else:\n",
    "        for i in eval(str(state['response'])):\n",
    "            if i not in [state['ground_truth']]:\n",
    "                return {\"next\": \"check\"}\n",
    "    return {\"next\": \"end\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0afcf905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benign_prompt_update(state: classification_state):\n",
    "    description= state['description']\n",
    "    messages = [\n",
    "    SystemMessage(content=f\"You are a ACMG Attribute classifier prompt developer for the given description, your task is to update the prompt to add the additional details which will correct your llm Prediction as your current prompt is returning {state['response']} but in reality it's {state['ground_truth']}, here is the reason why your LLM gave the old response {state['reason']}, now update the system prompt to get the correct response for the upcoming description and in response only return the prompt no extra text, give in points and include the previous prompts, here is the old prompt {state['benign_prompt']} and summarize it. Dont use any markdown characters, only text, make it as detail as possible so that that prompt i can use for any other description classification also, like a generalized prompt for the classification, and make sure i will only use this for benign classification\"),\n",
    "    HumanMessage(content=f\"\"\" Here is the given description:{description}\n",
    "    ########### respond only in the structure format:\n",
    "        prompt: str = Attribute Prompt\n",
    "    \"\"\")\n",
    "    ]\n",
    "    response = prompt_model.invoke(messages)\n",
    "    itter = state['current_iter']+1\n",
    "    return {\"benign_prompt\": response.prompt,\"current_iter\": itter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "790adbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathogenic_prompt_update(state: classification_state):\n",
    "    description= state['description']\n",
    "    messages = [\n",
    "    SystemMessage(content=f\"You are a ACMG Attribute classifier prompt developer for the given description, your task is to update the prompt to add the additional details which will correct your llm Prediction as your current prompt is returning {state['response']} but in reality it's {state['ground_truth']}, here is the reason why your LLM gave the old response {state['reason']}, now update the system prompt to get the correct response for the upcoming description and in response only return the prompt no extra text, give in points and include the previous prompts, here is the old prompt {state['pathogenic_prompt']} and summarize it. Dont use any markdown characters, only text,  make sure only prompt for pathogenic classification , make it as detail as possible so that that prompt i can use for any other description classification also, like a generalized prompt for the classification, and make sure i will only use this for pathogenic classification\"),\n",
    "    HumanMessage(content=f\"\"\" Here is the given description:{description}\n",
    "    ########### respond only in the structure format:\n",
    "        prompt: str = Attribute Prompt\n",
    "    \"\"\")\n",
    "    ]\n",
    "    response = prompt_model.invoke(messages)\n",
    "    itter = state['current_iter']+1\n",
    "    return {\"pathogenic_prompt\": response.prompt,\"current_iter\": itter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8febf973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def both_prompt_update(state: classification_state):\n",
    "    description= state['description']\n",
    "    messages = [\n",
    "    SystemMessage(content=f\"You are a ACMG Attribute classifier prompt developer for the given description, your task is to update the prompt to add the additional details which will correct your llm Prediction as your current prompt is returning {state['response']} but in reality it's {state['ground_truth']}, here is the reason why your LLM gave the old response {state['reason']}, now update the system prompt to get the correct response for the upcoming description and in response only return the prompt no extra text, give in points and include the previous prompts, here is the old prompt {state['both_prompt']} and summarize it. Dont use any markdown characters, only text\"),\n",
    "    HumanMessage(content=f\"\"\" Here is the given description:{description}\n",
    "    ########### respond only in the structure format:\n",
    "        prompt: str = Attribute Prompt\n",
    "    \"\"\")\n",
    "    ]\n",
    "    response = prompt_model.invoke(messages)\n",
    "    itter = state['current_iter']+1\n",
    "    return {\"both_prompt\": response.prompt,\"current_iter\": itter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a24f8c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_update(state: classification_state):\n",
    "    description= state['description']\n",
    "    messages = [\n",
    "    SystemMessage(content=f\"You are a ACMG Attribute classifier prompt developer for the given description, your task is to update the prompt to add the additional details which will correct your llm Prediction as your current prompt is returning {state['class_category']} but in reality it's {state['class_truth']}, here is the reason why your LLM gave the old response {state['class_category_reason']}, now update the system prompt to get the correct response for the upcoming description\"),\n",
    "    HumanMessage(content=f\"\"\" Here is the given description:{description}\n",
    "    ########### respond only in the structure format:\n",
    "        prompt: str = Attribute Prompt\n",
    "    \"\"\")\n",
    "    ]\n",
    "    response = prompt_model.invoke(messages)\n",
    "    itter = state['class_iter']+1\n",
    "    return {\"classification_prompt\": response.prompt,\"class_iter\": itter}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4193774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def workflow_selection(state: classification_state):\n",
    "    workflow = {\"next\":\"\"}\n",
    "    if state['class_category'] == 'Benign':\n",
    "        workflow['next'] = \"benign_classification\"\n",
    "    elif state['class_category'] == 'Pathogenic':\n",
    "        workflow['next'] = 'pathogenic_classification'\n",
    "    elif state['class_category'] == 'Both':\n",
    "        workflow['next'] = \"both_classification\"\n",
    "    else:\n",
    "        workflow['next'] = \"end\"\n",
    "    return workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "feb59dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(classification_state)\n",
    "\n",
    "graph.add_node('description_classification', description_classification)\n",
    "graph.add_node('classification_check',classification_check)\n",
    "graph.add_node('prompt_update',prompt_update)\n",
    "graph.add_node('workflow_selection', workflow_selection)\n",
    "\n",
    "graph.add_node('benign_classification', benign_classification)\n",
    "graph.add_node('pathogenic_classification', pathogenic_classification)\n",
    "graph.add_node('both_classification', both_classification)\n",
    "\n",
    "graph.add_node('benign_response_check', response_check)\n",
    "graph.add_node('pathogenic_response_check', response_check)\n",
    "graph.add_node('both_response_check', response_check)\n",
    "\n",
    "graph.add_node('benign_prompt_update', benign_prompt_update)\n",
    "graph.add_node('pathogenic_prompt_update', pathogenic_prompt_update)\n",
    "graph.add_node('both_prompt_update', both_prompt_update)\n",
    "\n",
    "graph.add_edge(START, 'description_classification')\n",
    "graph.add_edge('description_classification','classification_check')\n",
    "graph.add_conditional_edges('classification_check',lambda x: x[\"next\"],{\"continue\":\"workflow_selection\",\"check\":\"prompt_update\"})\n",
    "\n",
    "graph.add_edge('prompt_update','description_classification')\n",
    "\n",
    "graph.add_conditional_edges('workflow_selection',lambda x: x[\"next\"],\n",
    "    {\"end\": END, \"benign_classification\": \"benign_classification\",\"pathogenic_classification\":\"pathogenic_classification\",\"both_classification\":\"both_classification\"})\n",
    "\n",
    "graph.add_edge('both_classification','both_response_check')\n",
    "graph.add_edge('benign_classification','benign_response_check')\n",
    "graph.add_edge('pathogenic_classification','pathogenic_response_check')\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    'benign_response_check',\n",
    "    lambda x: x[\"next\"],\n",
    "    {\"end\": END, \"check\": \"benign_prompt_update\"}\n",
    ")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    'pathogenic_response_check',\n",
    "    lambda x: x[\"next\"],\n",
    "    {\"end\": END, \"check\": \"pathogenic_prompt_update\"}\n",
    ")\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    'both_response_check',\n",
    "    lambda x: x[\"next\"],\n",
    "    {\"end\": END, \"check\": \"both_prompt_update\"}\n",
    ")\n",
    "\n",
    "\n",
    "graph.add_edge('benign_prompt_update', 'benign_classification')\n",
    "graph.add_edge('pathogenic_prompt_update', 'pathogenic_classification')\n",
    "graph.add_edge('both_prompt_update', 'both_classification')\n",
    "\n",
    "workflow = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7cc8cedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rishi\\AppData\\Local\\Temp\\ipykernel_3748\\2288021753.py:3: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_balanced = df.groupby('class', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('final_classification.csv')\n",
    "\n",
    "df_balanced = df.groupby('class', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=33, random_state=42)\n",
    ")\n",
    "df_random = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1312164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classification_prompt.txt','r') as file:\n",
    "    classification_prompt = file.readlines()\n",
    "with open('benign_prompt.txt','r') as file:\n",
    "    benign_prompt = file.readlines()\n",
    "with open('pathogenic_prompt.txt','r') as file:\n",
    "    pathogenic_prompt = file.readlines()\n",
    "with open('both_prompt.txt','r') as file:\n",
    "    both_prompt = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f64fad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = {'description': 'The p.V217I variant (also known as c.649G>A), located in coding exon 7 of the PTEN gene, results from a G to A substitution at nucleotide position 649. The valine at codon 217 is replaced by isoleucine, an amino acid with highly similar properties. In a massively parallel functional assay using a humanized yeast model, lipid phosphatase activity for this variant was functionally neutral (Mighell TL et al. Am J Hum Genet, 2018 May;102:943-955). This variant demonstrated wild type-like intracellular protein abundance in a massively parallel functional assay (Matreyek KA et al. Nat Genet, 2018 Jun;50:874-882). This amino acid position is highly conserved in available vertebrate species. In addition, this alteration is predicted to be tolerated by in silico analysis. Since supporting evidence is limited at this time, the clinical significance of this alteration remains unclear.',\n",
    " 'current_iter': 0,\n",
    " 'max_iter': 3,\n",
    " 'ground_truth': \"['BS3']\",\n",
    " 'benign_prompt': benign_prompt,\n",
    " 'pathogenic_prompt': pathogenic_prompt,\n",
    " 'both_prompt': both_prompt,\n",
    " 'initial_state': 0,\n",
    " 'class_iter': 0,\n",
    " 'classification_prompt': classification_prompt,\n",
    " 'class_truth': 'Benign'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1e991b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itter: 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-flash\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 250\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 20\n",
      "}\n",
      "].\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count = 51\n",
    "for i in df_random.values[52:]:\n",
    "    try:\n",
    "        count+=1\n",
    "        print(f\"Itter: {count}\")\n",
    "        res=workflow.invoke(initial_state)\n",
    "        initial_state['initial_state'] = 0\n",
    "        initial_state['class_iter'] = 0\n",
    "        initial_state['ground_truth'] = i[3]\n",
    "        initial_state['description'] = i[2]\n",
    "        initial_state['benign_prompt'] = res['benign_prompt']\n",
    "        initial_state['pathogenic_prompt'] = res['pathogenic_prompt']\n",
    "        initial_state['both_prompt'] = res['both_prompt']\n",
    "        initial_state['classification_prompt']=res['classification_prompt']\n",
    "        initial_state['class_truth']=i[4]\n",
    "        with open('benign_prompt.txt','w') as file:\n",
    "            file.write(initial_state['benign_prompt'])\n",
    "        with open('pathogenic_prompt.txt','w') as file:\n",
    "            file.write(initial_state['pathogenic_prompt'])\n",
    "        with open('both_prompt.txt','w') as file:\n",
    "            file.write(initial_state['both_prompt'])\n",
    "        with open('classification_prompt.txt','w') as file:\n",
    "            file.write(initial_state['classification_prompt'])\n",
    "        if count%10==0:\n",
    "            time.sleep(10)\n",
    "    except Exception as e:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03273c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e7c450",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57bfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
